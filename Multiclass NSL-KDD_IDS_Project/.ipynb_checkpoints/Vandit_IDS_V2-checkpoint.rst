.. code:: ipython3

    # Importing libraries
    
    # For Data Analysis and Manipulation
    import pandas as pd
    import math
    import numpy as np
    
    # For Data Visualization
    import matplotlib.pyplot as plt
    import seaborn as sns
    # For analysing time
    import time
    # For Data Scaling and Encoding
    from sklearn.preprocessing import StandardScaler
    from tensorflow.keras.utils import to_categorical
    from sklearn.preprocessing import LabelEncoder
    
    # For Preprocessing and modelling
    from tensorflow.keras.layers import Input
    from sklearn.preprocessing import FunctionTransformer
    import tensorflow as tf
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    from xgboost import XGBClassifier
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, Dense, BatchNormalization, GlobalAveragePooling1D
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.optimizers import Adam

.. code:: ipython3

    # Downloading training and test files
    !wget https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt
    !wget https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest+.txt


.. parsed-literal::

    --2025-08-17 13:36:03--  https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt
    Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
    Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 19109424 (18M) [text/plain]
    Saving to: 'KDDTrain+.txt'
    
         0K .......... .......... .......... .......... ..........  0% 2.52M 7s
        50K .......... .......... .......... .......... ..........  0% 4.96M 5s
       100K .......... .......... .......... .......... ..........  0% 6.11M 5s
       150K .......... .......... .......... .......... ..........  1% 5.49M 4s
       200K .......... .......... .......... .......... ..........  1% 7.41M 4s
       250K .......... .......... .......... .......... ..........  1% 22.9M 3s
       300K .......... .......... .......... .......... ..........  1% 23.8M 3s
       350K .......... .......... .......... .......... ..........  2% 16.4M 3s
       400K .......... .......... .......... .......... ..........  2% 6.57M 3s
       450K .......... .......... .......... .......... ..........  2% 47.8M 2s
       500K .......... .......... .......... .......... ..........  2% 34.4M 2s
       550K .......... .......... .......... .......... ..........  3% 13.7M 2s
       600K .......... .......... .......... .......... ..........  3% 21.8M 2s
       650K .......... .......... .......... .......... ..........  3% 40.0M 2s
       700K .......... .......... .......... .......... ..........  4% 48.6M 2s
       750K .......... .......... .......... .......... ..........  4% 27.1M 2s
       800K .......... .......... .......... .......... ..........  4%  453M 2s
       850K .......... .......... .......... .......... ..........  4% 11.6M 2s
       900K .......... .......... .......... .......... ..........  5% 31.1M 2s
       950K .......... .......... .......... .......... ..........  5% 41.9M 2s
      1000K .......... .......... .......... .......... ..........  5% 52.3M 1s
      1050K .......... .......... .......... .......... ..........  5% 31.2M 1s
      1100K .......... .......... .......... .......... ..........  6% 50.0M 1s
      1150K .......... .......... .......... .......... ..........  6% 27.2M 1s
      1200K .......... .......... .......... .......... ..........  6% 42.9M 1s
      1250K .......... .......... .......... .......... ..........  6% 35.6M 1s
      1300K .......... .......... .......... .......... ..........  7% 35.0M 1s
      1350K .......... .......... .......... .......... ..........  7%  628M 1s
      1400K .......... .......... .......... .......... ..........  7% 41.9M 1s
      1450K .......... .......... .......... .......... ..........  8% 16.7M 1s
      1500K .......... .......... .......... .......... ..........  8%  253M 1s
      1550K .......... .......... .......... .......... ..........  8%  667M 1s
      1600K .......... .......... .......... .......... ..........  8% 71.2M 1s
      1650K .......... .......... .......... .......... ..........  9% 77.2M 1s
      1700K .......... .......... .......... .......... ..........  9% 17.9M 1s
      1750K .......... .......... .......... .......... ..........  9% 34.8M 1s
      1800K .......... .......... .......... .......... ..........  9%  478M 1s
      1850K .......... .......... .......... .......... .......... 10% 35.1M 1s
      1900K .......... .......... .......... .......... .......... 10% 63.0M 1s
      1950K .......... .......... .......... .......... .......... 10% 13.4M 1s
      2000K .......... .......... .......... .......... .......... 10%  469M 1s
      2050K .......... .......... .......... .......... .......... 11%  238M 1s
      2100K .......... .......... .......... .......... .......... 11%  610M 1s
      2150K .......... .......... .......... .......... .......... 11% 18.0M 1s
      2200K .......... .......... .......... .......... .......... 12%  324M 1s
      2250K .......... .......... .......... .......... .......... 12%  604M 1s
      2300K .......... .......... .......... .......... .......... 12%  418M 1s
      2350K .......... .......... .......... .......... .......... 12% 52.7M 1s
      2400K .......... .......... .......... .......... .......... 13% 61.0M 1s
      2450K .......... .......... .......... .......... .......... 13% 12.6M 1s
      2500K .......... .......... .......... .......... .......... 13%  324M 1s
      2550K .......... .......... .......... .......... .......... 13%  583M 1s
      2600K .......... .......... .......... .......... .......... 14%  287M 1s
      2650K .......... .......... .......... .......... .......... 14% 28.4M 1s
      2700K .......... .......... .......... .......... .......... 14%  693M 1s
      2750K .......... .......... .......... .......... .......... 15% 3.27M 1s
      2800K .......... .......... .......... .......... .......... 15% 13.8M 1s
      2850K .......... .......... .......... .......... .......... 15%  486M 1s
      2900K .......... .......... .......... .......... .......... 15%  708M 1s
      2950K .......... .......... .......... .......... .......... 16%  360M 1s
      3000K .......... .......... .......... .......... .......... 16%  241M 1s
      3050K .......... .......... .......... .......... .......... 16%  448M 1s
      3100K .......... .......... .......... .......... .......... 16% 9.40M 1s
      3150K .......... .......... .......... .......... .......... 17%  183M 1s
      3200K .......... .......... .......... .......... .......... 17%  581M 1s
      3250K .......... .......... .......... .......... .......... 17%  663M 1s
      3300K .......... .......... .......... .......... .......... 17%  623M 1s
      3350K .......... .......... .......... .......... .......... 18%  238M 1s
      3400K .......... .......... .......... .......... .......... 18%  447M 1s
      3450K .......... .......... .......... .......... .......... 18%  609M 1s
      3500K .......... .......... .......... .......... .......... 19%  402M 1s
      3550K .......... .......... .......... .......... .......... 19%  301M 1s
      3600K .......... .......... .......... .......... .......... 19%  286M 1s
      3650K .......... .......... .......... .......... .......... 19%  420M 1s
      3700K .......... .......... .......... .......... .......... 20% 9.98M 1s
      3750K .......... .......... .......... .......... .......... 20%  421M 1s
      3800K .......... .......... .......... .......... .......... 20%  556M 1s
      3850K .......... .......... .......... .......... .......... 20%  432M 1s
      3900K .......... .......... .......... .......... .......... 21%  359M 1s
      3950K .......... .......... .......... .......... .......... 21%  313M 1s
      4000K .......... .......... .......... .......... .......... 21%  692M 1s
      4050K .......... .......... .......... .......... .......... 21%  673M 1s
      4100K .......... .......... .......... .......... .......... 22%  324M 1s
      4150K .......... .......... .......... .......... .......... 22%  479M 1s
      4200K .......... .......... .......... .......... .......... 22%  707M 1s
      4250K .......... .......... .......... .......... .......... 23%  355M 1s
      4300K .......... .......... .......... .......... .......... 23%  619M 1s
      4350K .......... .......... .......... .......... .......... 23%  709M 1s
      4400K .......... .......... .......... .......... .......... 23%  481M 0s
      4450K .......... .......... .......... .......... .......... 24%  465M 0s
      4500K .......... .......... .......... .......... .......... 24% 23.5M 0s
      4550K .......... .......... .......... .......... .......... 24%  554M 0s
      4600K .......... .......... .......... .......... .......... 24%  608M 0s
      4650K .......... .......... .......... .......... .......... 25%  292M 0s
      4700K .......... .......... .......... .......... .......... 25%  581M 0s
      4750K .......... .......... .......... .......... .......... 25%  618M 0s
      4800K .......... .......... .......... .......... .......... 25% 13.0M 0s
      4850K .......... .......... .......... .......... .......... 26% 20.5M 0s
      4900K .......... .......... .......... .......... .......... 26% 31.9M 0s
      4950K .......... .......... .......... .......... .......... 26%  459M 0s
      5000K .......... .......... .......... .......... .......... 27% 24.4M 0s
      5050K .......... .......... .......... .......... .......... 27%  559M 0s
      5100K .......... .......... .......... .......... .......... 27% 31.7M 0s
      5150K .......... .......... .......... .......... .......... 27%  609M 0s
      5200K .......... .......... .......... .......... .......... 28%  431M 0s
      5250K .......... .......... .......... .......... .......... 28% 9.39M 0s
      5300K .......... .......... .......... .......... .......... 28%  377M 0s
      5350K .......... .......... .......... .......... .......... 28%  336M 0s
      5400K .......... .......... .......... .......... .......... 29% 41.2M 0s
      5450K .......... .......... .......... .......... .......... 29% 16.7M 0s
      5500K .......... .......... .......... .......... .......... 29%  474M 0s
      5550K .......... .......... .......... .......... .......... 30%  731M 0s
      5600K .......... .......... .......... .......... .......... 30%  290M 0s
      5650K .......... .......... .......... .......... .......... 30% 16.7M 0s
      5700K .......... .......... .......... .......... .......... 30%  341M 0s
      5750K .......... .......... .......... .......... .......... 31%  383M 0s
      5800K .......... .......... .......... .......... .......... 31%  503M 0s
      5850K .......... .......... .......... .......... .......... 31%  580M 0s
      5900K .......... .......... .......... .......... .......... 31%  447M 0s
      5950K .......... .......... .......... .......... .......... 32% 20.7M 0s
      6000K .......... .......... .......... .......... .......... 32%  610M 0s
      6050K .......... .......... .......... .......... .......... 32%  470M 0s
      6100K .......... .......... .......... .......... .......... 32%  433M 0s
      6150K .......... .......... .......... .......... .......... 33% 12.7M 0s
      6200K .......... .......... .......... .......... .......... 33%  537M 0s
      6250K .......... .......... .......... .......... .......... 33%  574M 0s
      6300K .......... .......... .......... .......... .......... 34%  449M 0s
      6350K .......... .......... .......... .......... .......... 34%  531M 0s
      6400K .......... .......... .......... .......... .......... 34%  157M 0s
      6450K .......... .......... .......... .......... .......... 34%  493M 0s
      6500K .......... .......... .......... .......... .......... 35% 13.6M 0s
      6550K .......... .......... .......... .......... .......... 35%  594M 0s
      6600K .......... .......... .......... .......... .......... 35%  541M 0s
      6650K .......... .......... .......... .......... .......... 35%  628M 0s
      6700K .......... .......... .......... .......... .......... 36%  200M 0s
      6750K .......... .......... .......... .......... .......... 36%  586M 0s
      6800K .......... .......... .......... .......... .......... 36%  730M 0s
      6850K .......... .......... .......... .......... .......... 36% 36.6M 0s
      6900K .......... .......... .......... .......... .......... 37%  304M 0s
      6950K .......... .......... .......... .......... .......... 37%  705M 0s
      7000K .......... .......... .......... .......... .......... 37% 12.5M 0s
      7050K .......... .......... .......... .......... .......... 38%  400M 0s
      7100K .......... .......... .......... .......... .......... 38%  315M 0s
      7150K .......... .......... .......... .......... .......... 38%  466M 0s
      7200K .......... .......... .......... .......... .......... 38%  594M 0s
      7250K .......... .......... .......... .......... .......... 39%  402M 0s
      7300K .......... .......... .......... .......... .......... 39%  176M 0s
      7350K .......... .......... .......... .......... .......... 39% 13.3M 0s
      7400K .......... .......... .......... .......... .......... 39%  543M 0s
      7450K .......... .......... .......... .......... .......... 40%  273M 0s
      7500K .......... .......... .......... .......... .......... 40% 10.3M 0s
      7550K .......... .......... .......... .......... .......... 40%  642M 0s
      7600K .......... .......... .......... .......... .......... 40%  353M 0s
      7650K .......... .......... .......... .......... .......... 41%  252M 0s
      7700K .......... .......... .......... .......... .......... 41%  471M 0s
      7750K .......... .......... .......... .......... .......... 41%  629M 0s
      7800K .......... .......... .......... .......... .......... 42%  268M 0s
      7850K .......... .......... .......... .......... .......... 42%  327M 0s
      7900K .......... .......... .......... .......... .......... 42%  660M 0s
      7950K .......... .......... .......... .......... .......... 42% 9.49M 0s
      8000K .......... .......... .......... .......... .......... 43%  333M 0s
      8050K .......... .......... .......... .......... .......... 43%  427M 0s
      8100K .......... .......... .......... .......... .......... 43%  710M 0s
      8150K .......... .......... .......... .......... .......... 43%  709M 0s
      8200K .......... .......... .......... .......... .......... 44%  397M 0s
      8250K .......... .......... .......... .......... .......... 44%  370M 0s
      8300K .......... .......... .......... .......... .......... 44%  713M 0s
      8350K .......... .......... .......... .......... .......... 45%  723M 0s
      8400K .......... .......... .......... .......... .......... 45%  413M 0s
      8450K .......... .......... .......... .......... .......... 45%  462M 0s
      8500K .......... .......... .......... .......... .......... 45%  729M 0s
      8550K .......... .......... .......... .......... .......... 46% 23.2M 0s
      8600K .......... .......... .......... .......... .......... 46%  588M 0s
      8650K .......... .......... .......... .......... .......... 46%  686M 0s
      8700K .......... .......... .......... .......... .......... 46%  362M 0s
      8750K .......... .......... .......... .......... .......... 47%  286M 0s
      8800K .......... .......... .......... .......... .......... 47% 12.5M 0s
      8850K .......... .......... .......... .......... .......... 47%  358M 0s
      8900K .......... .......... .......... .......... .......... 47%  554M 0s
      8950K .......... .......... .......... .......... .......... 48%  288M 0s
      9000K .......... .......... .......... .......... .......... 48%  408M 0s
      9050K .......... .......... .......... .......... .......... 48%  637M 0s
      9100K .......... .......... .......... .......... .......... 49%  396M 0s
      9150K .......... .......... .......... .......... .......... 49% 13.4M 0s
      9200K .......... .......... .......... .......... .......... 49%  428M 0s
      9250K .......... .......... .......... .......... .......... 49%  660M 0s
      9300K .......... .......... .......... .......... .......... 50%  429M 0s
      9350K .......... .......... .......... .......... .......... 50%  316M 0s
      9400K .......... .......... .......... .......... .......... 50%  507M 0s
      9450K .......... .......... .......... .......... .......... 50%  618M 0s
      9500K .......... .......... .......... .......... .......... 51% 10.1M 0s
      9550K .......... .......... .......... .......... .......... 51%  659M 0s
      9600K .......... .......... .......... .......... .......... 51%  704M 0s
      9650K .......... .......... .......... .......... .......... 51%  287M 0s
      9700K .......... .......... .......... .......... .......... 52%  318M 0s
      9750K .......... .......... .......... .......... .......... 52%  689M 0s
      9800K .......... .......... .......... .......... .......... 52%  404M 0s
      9850K .......... .......... .......... .......... .......... 53%  408M 0s
      9900K .......... .......... .......... .......... .......... 53%  285M 0s
      9950K .......... .......... .......... .......... .......... 53%  502M 0s
     10000K .......... .......... .......... .......... .......... 53% 10.0M 0s
     10050K .......... .......... .......... .......... .......... 54%  233M 0s
     10100K .......... .......... .......... .......... .......... 54%  660M 0s
     10150K .......... .......... .......... .......... .......... 54%  676M 0s
     10200K .......... .......... .......... .......... .......... 54%  681M 0s
     10250K .......... .......... .......... .......... .......... 55%  393M 0s
     10300K .......... .......... .......... .......... .......... 55%  270M 0s
     10350K .......... .......... .......... .......... .......... 55%  625M 0s
     10400K .......... .......... .......... .......... .......... 55%  484M 0s
     10450K .......... .......... .......... .......... .......... 56%  356M 0s
     10500K .......... .......... .......... .......... .......... 56%  702M 0s
     10550K .......... .......... .......... .......... .......... 56%  559M 0s
     10600K .......... .......... .......... .......... .......... 57% 13.9M 0s
     10650K .......... .......... .......... .......... .......... 57%  515M 0s
     10700K .......... .......... .......... .......... .......... 57%  671M 0s
     10750K .......... .......... .......... .......... .......... 57%  328M 0s
     10800K .......... .......... .......... .......... .......... 58%  286M 0s
     10850K .......... .......... .......... .......... .......... 58%  676M 0s
     10900K .......... .......... .......... .......... .......... 58%  713M 0s
     10950K .......... .......... .......... .......... .......... 58% 32.3M 0s
     11000K .......... .......... .......... .......... .......... 59%  370M 0s
     11050K .......... .......... .......... .......... .......... 59%  404M 0s
     11100K .......... .......... .......... .......... .......... 59% 11.6M 0s
     11150K .......... .......... .......... .......... .......... 60%  427M 0s
     11200K .......... .......... .......... .......... .......... 60%  581M 0s
     11250K .......... .......... .......... .......... .......... 60%  532M 0s
     11300K .......... .......... .......... .......... .......... 60%  441M 0s
     11350K .......... .......... .......... .......... .......... 61%  251M 0s
     11400K .......... .......... .......... .......... .......... 61%  220M 0s
     11450K .......... .......... .......... .......... .......... 61% 10.8M 0s
     11500K .......... .......... .......... .......... .......... 61%  540M 0s
     11550K .......... .......... .......... .......... .......... 62%  260M 0s
     11600K .......... .......... .......... .......... .......... 62%  455M 0s
     11650K .......... .......... .......... .......... .......... 62%  436M 0s
     11700K .......... .......... .......... .......... .......... 62%  585M 0s
     11750K .......... .......... .......... .......... .......... 63%  246M 0s
     11800K .......... .......... .......... .......... .......... 63%  446M 0s
     11850K .......... .......... .......... .......... .......... 63%  535M 0s
     11900K .......... .......... .......... .......... .......... 64%  366M 0s
     11950K .......... .......... .......... .......... .......... 64% 46.6M 0s
     12000K .......... .......... .......... .......... .......... 64% 14.7M 0s
     12050K .......... .......... .......... .......... .......... 64%  583M 0s
     12100K .......... .......... .......... .......... .......... 65%  214M 0s
     12150K .......... .......... .......... .......... .......... 65%  662M 0s
     12200K .......... .......... .......... .......... .......... 65%  699M 0s
     12250K .......... .......... .......... .......... .......... 65% 9.07M 0s
     12300K .......... .......... .......... .......... .......... 66%  341M 0s
     12350K .......... .......... .......... .......... .......... 66%  509M 0s
     12400K .......... .......... .......... .......... .......... 66%  410M 0s
     12450K .......... .......... .......... .......... .......... 66%  521M 0s
     12500K .......... .......... .......... .......... .......... 67%  381M 0s
     12550K .......... .......... .......... .......... .......... 67%  661M 0s
     12600K .......... .......... .......... .......... .......... 67%  542M 0s
     12650K .......... .......... .......... .......... .......... 68%  321M 0s
     12700K .......... .......... .......... .......... .......... 68%  631M 0s
     12750K .......... .......... .......... .......... .......... 68%  735M 0s
     12800K .......... .......... .......... .......... .......... 68%  711M 0s
     12850K .......... .......... .......... .......... .......... 69% 14.3M 0s
     12900K .......... .......... .......... .......... .......... 69%  376M 0s
     12950K .......... .......... .......... .......... .......... 69%  581M 0s
     13000K .......... .......... .......... .......... .......... 69%  586M 0s
     13050K .......... .......... .......... .......... .......... 70%  176M 0s
     13100K .......... .......... .......... .......... .......... 70%  486M 0s
     13150K .......... .......... .......... .......... .......... 70%  615M 0s
     13200K .......... .......... .......... .......... .......... 71% 10.6M 0s
     13250K .......... .......... .......... .......... .......... 71%  359M 0s
     13300K .......... .......... .......... .......... .......... 71%  509M 0s
     13350K .......... .......... .......... .......... .......... 71%  341M 0s
     13400K .......... .......... .......... .......... .......... 72%  393M 0s
     13450K .......... .......... .......... .......... .......... 72%  519M 0s
     13500K .......... .......... .......... .......... .......... 72%  417M 0s
     13550K .......... .......... .......... .......... .......... 72%  645M 0s
     13600K .......... .......... .......... .......... .......... 73%  241M 0s
     13650K .......... .......... .......... .......... .......... 73%  581M 0s
     13700K .......... .......... .......... .......... .......... 73% 9.92M 0s
     13750K .......... .......... .......... .......... .......... 73%  344M 0s
     13800K .......... .......... .......... .......... .......... 74%  376M 0s
     13850K .......... .......... .......... .......... .......... 74%  419M 0s
     13900K .......... .......... .......... .......... .......... 74%  509M 0s
     13950K .......... .......... .......... .......... .......... 75%  619M 0s
     14000K .......... .......... .......... .......... .......... 75%  268M 0s
     14050K .......... .......... .......... .......... .......... 75%  388M 0s
     14100K .......... .......... .......... .......... .......... 75%  488M 0s
     14150K .......... .......... .......... .......... .......... 76%  229M 0s
     14200K .......... .......... .......... .......... .......... 76%  478M 0s
     14250K .......... .......... .......... .......... .......... 76% 18.4M 0s
     14300K .......... .......... .......... .......... .......... 76%  392M 0s
     14350K .......... .......... .......... .......... .......... 77%  361M 0s
     14400K .......... .......... .......... .......... .......... 77%  216M 0s
     14450K .......... .......... .......... .......... .......... 77%  263M 0s
     14500K .......... .......... .......... .......... .......... 77%  428M 0s
     14550K .......... .......... .......... .......... .......... 78% 9.91M 0s
     14600K .......... .......... .......... .......... .......... 78%  569M 0s
     14650K .......... .......... .......... .......... .......... 78%  302M 0s
     14700K .......... .......... .......... .......... .......... 79%  591M 0s
     14750K .......... .......... .......... .......... .......... 79%  373M 0s
     14800K .......... .......... .......... .......... .......... 79%  518M 0s
     14850K .......... .......... .......... .......... .......... 79%  682M 0s
     14900K .......... .......... .......... .......... .......... 80%  373M 0s
     14950K .......... .......... .......... .......... .......... 80%  326M 0s
     15000K .......... .......... .......... .......... .......... 80% 9.68M 0s
     15050K .......... .......... .......... .......... .......... 80%  422M 0s
     15100K .......... .......... .......... .......... .......... 81%  335M 0s
     15150K .......... .......... .......... .......... .......... 81%  504M 0s
     15200K .......... .......... .......... .......... .......... 81%  374M 0s
     15250K .......... .......... .......... .......... .......... 81%  678M 0s
     15300K .......... .......... .......... .......... .......... 82%  421M 0s
     15350K .......... .......... .......... .......... .......... 82%  529M 0s
     15400K .......... .......... .......... .......... .......... 82%  676M 0s
     15450K .......... .......... .......... .......... .......... 83%  680M 0s
     15500K .......... .......... .......... .......... .......... 83%  174M 0s
     15550K .......... .......... .......... .......... .......... 83%  440M 0s
     15600K .......... .......... .......... .......... .......... 83%  669M 0s
     15650K .......... .......... .......... .......... .......... 84% 10.3M 0s
     15700K .......... .......... .......... .......... .......... 84%  533M 0s
     15750K .......... .......... .......... .......... .......... 84%  669M 0s
     15800K .......... .......... .......... .......... .......... 84%  347M 0s
     15850K .......... .......... .......... .......... .......... 85%  301M 0s
     15900K .......... .......... .......... .......... .......... 85%  428M 0s
     15950K .......... .......... .......... .......... .......... 85%  663M 0s
     16000K .......... .......... .......... .......... .......... 86%  161M 0s
     16050K .......... .......... .......... .......... .......... 86%  337M 0s
     16100K .......... .......... .......... .......... .......... 86%  640M 0s
     16150K .......... .......... .......... .......... .......... 86%  647M 0s
     16200K .......... .......... .......... .......... .......... 87% 11.3M 0s
     16250K .......... .......... .......... .......... .......... 87%  278M 0s
     16300K .......... .......... .......... .......... .......... 87%  663M 0s
     16350K .......... .......... .......... .......... .......... 87%  348M 0s
     16400K .......... .......... .......... .......... .......... 88%  433M 0s
     16450K .......... .......... .......... .......... .......... 88%  595M 0s
     16500K .......... .......... .......... .......... .......... 88%  311M 0s
     16550K .......... .......... .......... .......... .......... 88%  592M 0s
     16600K .......... .......... .......... .......... .......... 89%  279M 0s
     16650K .......... .......... .......... .......... .......... 89%  627M 0s
     16700K .......... .......... .......... .......... .......... 89% 10.3M 0s
     16750K .......... .......... .......... .......... .......... 90%  405M 0s
     16800K .......... .......... .......... .......... .......... 90%  343M 0s
     16850K .......... .......... .......... .......... .......... 90%  249M 0s
     16900K .......... .......... .......... .......... .......... 90%  587M 0s
     16950K .......... .......... .......... .......... .......... 91%  256M 0s
     17000K .......... .......... .......... .......... .......... 91%  380M 0s
     17050K .......... .......... .......... .......... .......... 91%  626M 0s
     17100K .......... .......... .......... .......... .......... 91%  684M 0s
     17150K .......... .......... .......... .......... .......... 92% 9.71M 0s
     17200K .......... .......... .......... .......... .......... 92%  386M 0s
     17250K .......... .......... .......... .......... .......... 92%  341M 0s
     17300K .......... .......... .......... .......... .......... 92%  384M 0s
     17350K .......... .......... .......... .......... .......... 93%  329M 0s
     17400K .......... .......... .......... .......... .......... 93%  300M 0s
     17450K .......... .......... .......... .......... .......... 93%  297M 0s
     17500K .......... .......... .......... .......... .......... 94%  422M 0s
     17550K .......... .......... .......... .......... .......... 94%  574M 0s
     17600K .......... .......... .......... .......... .......... 94%  704M 0s
     17650K .......... .......... .......... .......... .......... 94%  689M 0s
     17700K .......... .......... .......... .......... .......... 95% 12.5M 0s
     17750K .......... .......... .......... .......... .......... 95%  498M 0s
     17800K .......... .......... .......... .......... .......... 95%  335M 0s
     17850K .......... .......... .......... .......... .......... 95%  459M 0s
     17900K .......... .......... .......... .......... .......... 96%  388M 0s
     17950K .......... .......... .......... .......... .......... 96%  370M 0s
     18000K .......... .......... .......... .......... .......... 96%  402M 0s
     18050K .......... .......... .......... .......... .......... 96%  547M 0s
     18100K .......... .......... .......... .......... .......... 97% 3.96M 0s
     18150K .......... .......... .......... .......... .......... 97%  292M 0s
     18200K .......... .......... .......... .......... .......... 97%  706M 0s
     18250K .......... .......... .......... .......... .......... 98%  376M 0s
     18300K .......... .......... .......... .......... .......... 98%  419M 0s
     18350K .......... .......... .......... .......... .......... 98%  712M 0s
     18400K .......... .......... .......... .......... .......... 98%  383M 0s
     18450K .......... .......... .......... .......... .......... 99%  362M 0s
     18500K .......... .......... .......... .......... .......... 99%  579M 0s
     18550K .......... .......... .......... .......... .......... 99%  351M 0s
     18600K .......... .......... .......... .......... .......... 99%  731M 0s
     18650K .......... .                                          100%  520M=0.3s
    
    2025-08-17 13:36:03 (53.5 MB/s) - 'KDDTrain+.txt' saved [19109424/19109424]
    
    --2025-08-17 13:36:03--  https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest+.txt
    Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...
    Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 3441513 (3.3M) [text/plain]
    Saving to: 'KDDTest+.txt'
    
         0K .......... .......... .......... .......... ..........  1% 2.45M 1s
        50K .......... .......... .......... .......... ..........  2% 8.64M 1s
       100K .......... .......... .......... .......... ..........  4% 7.44M 1s
       150K .......... .......... .......... .......... ..........  5% 13.4M 1s
       200K .......... .......... .......... .......... ..........  7% 15.4M 0s
       250K .......... .......... .......... .......... ..........  8% 8.89M 0s
       300K .......... .......... .......... .......... .......... 10% 20.5M 0s
       350K .......... .......... .......... .......... .......... 11% 14.6M 0s
       400K .......... .......... .......... .......... .......... 13% 41.3M 0s
       450K .......... .......... .......... .......... .......... 14% 31.7M 0s
       500K .......... .......... .......... .......... .......... 16% 6.57M 0s
       550K .......... .......... .......... .......... .......... 17% 20.8M 0s
       600K .......... .......... .......... .......... .......... 19% 28.3M 0s
       650K .......... .......... .......... .......... .......... 20% 34.1M 0s
       700K .......... .......... .......... .......... .......... 22%  288M 0s
       750K .......... .......... .......... .......... .......... 23% 76.3M 0s
       800K .......... .......... .......... .......... .......... 25% 8.49M 0s
       850K .......... .......... .......... .......... .......... 26% 17.4M 0s
       900K .......... .......... .......... .......... .......... 28%  341M 0s
       950K .......... .......... .......... .......... .......... 29% 33.5M 0s
      1000K .......... .......... .......... .......... .......... 31% 37.9M 0s
      1050K .......... .......... .......... .......... .......... 32%  403M 0s
      1100K .......... .......... .......... .......... .......... 34% 30.8M 0s
      1150K .......... .......... .......... .......... .......... 35%  395M 0s
      1200K .......... .......... .......... .......... .......... 37% 59.9M 0s
      1250K .......... .......... .......... .......... .......... 38% 31.9M 0s
      1300K .......... .......... .......... .......... .......... 40%  416M 0s
      1350K .......... .......... .......... .......... .......... 41% 24.4M 0s
      1400K .......... .......... .......... .......... .......... 43% 28.1M 0s
      1450K .......... .......... .......... .......... .......... 44%  383M 0s
      1500K .......... .......... .......... .......... .......... 46%  468M 0s
      1550K .......... .......... .......... .......... .......... 47% 45.1M 0s
      1600K .......... .......... .......... .......... .......... 49% 35.2M 0s
      1650K .......... .......... .......... .......... .......... 50% 55.1M 0s
      1700K .......... .......... .......... .......... .......... 52% 34.6M 0s
      1750K .......... .......... .......... .......... .......... 53% 18.1M 0s
      1800K .......... .......... .......... .......... .......... 55%  257M 0s
      1850K .......... .......... .......... .......... .......... 56% 42.0M 0s
      1900K .......... .......... .......... .......... .......... 58% 19.5M 0s
      1950K .......... .......... .......... .......... .......... 59%  357M 0s
      2000K .......... .......... .......... .......... .......... 60%  303M 0s
      2050K .......... .......... .......... .......... .......... 62% 48.4M 0s
      2100K .......... .......... .......... .......... .......... 63% 20.0M 0s
      2150K .......... .......... .......... .......... .......... 65%  227M 0s
      2200K .......... .......... .......... .......... .......... 66%  272M 0s
      2250K .......... .......... .......... .......... .......... 68% 31.8M 0s
      2300K .......... .......... .......... .......... .......... 69%  338M 0s
      2350K .......... .......... .......... .......... .......... 71%  243M 0s
      2400K .......... .......... .......... .......... .......... 72% 17.6M 0s
      2450K .......... .......... .......... .......... .......... 74%  430M 0s
      2500K .......... .......... .......... .......... .......... 75%  211M 0s
      2550K .......... .......... .......... .......... .......... 77%  316M 0s
      2600K .......... .......... .......... .......... .......... 78% 55.1M 0s
      2650K .......... .......... .......... .......... .......... 80% 17.3M 0s
      2700K .......... .......... .......... .......... .......... 81%  278M 0s
      2750K .......... .......... .......... .......... .......... 83%  323M 0s
      2800K .......... .......... .......... .......... .......... 84%  357M 0s
      2850K .......... .......... .......... .......... .......... 86% 59.7M 0s
      2900K .......... .......... .......... .......... .......... 87% 15.4M 0s
      2950K .......... .......... .......... .......... .......... 89%  160M 0s
      3000K .......... .......... .......... .......... .......... 90%  294M 0s
      3050K .......... .......... .......... .......... .......... 92%  250M 0s
      3100K .......... .......... .......... .......... .......... 93%  279M 0s
      3150K .......... .......... .......... .......... .......... 95% 6.65M 0s
      3200K .......... .......... .......... .......... .......... 96%  356M 0s
      3250K .......... .......... .......... .......... .......... 98%  285M 0s
      3300K .......... .......... .......... .......... .......... 99%  357M 0s
      3350K ..........                                            100%  207G=0.1s
    
    2025-08-17 13:36:04 (26.5 MB/s) - 'KDDTest+.txt' saved [3441513/3441513]
    


.. code:: ipython3

    # Defining the 41 features + label
    columns = np.array(['duration','protocol_type','service','flag','src_bytes','dst_bytes','land',
      'wrong_fragment','urgent','hot','num_failed_logins','logged_in','num_compromised',
      'root_shell','su_attempted','num_root','num_file_creations','num_shells',
       'num_access_files','num_outbound_cmds','is_host_login','is_guest_login','count',
        'srv_count','serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate',
        'same_srv_rate','diff_srv_rate','srv_diff_host_rate','dst_host_count',
        'dst_host_srv_count','dst_host_same_srv_rate','dst_host_diff_srv_rate',
        'dst_host_same_src_port_rate','dst_host_srv_diff_host_rate','dst_host_serror_rate',
        'dst_host_srv_serror_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate',
        'label', 'difficulty level'])

.. code:: ipython3

    # Loading data into DataFrames
    trainingData = pd.read_csv("KDDTrain+.txt", names=columns)
    testingData  = pd.read_csv("KDDTest+.txt", names=columns)

.. code:: ipython3

    # Looking for total rows and columns in both data files
    print("Train shape:", trainingData.shape)
    print("Test  shape:", testingData.shape)


.. parsed-literal::

    Train shape: (125973, 43)
    Test  shape: (22544, 43)


.. code:: ipython3

    # Printing the first few lines of Training data
    trainingData.head()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>duration</th>
          <th>protocol_type</th>
          <th>service</th>
          <th>flag</th>
          <th>src_bytes</th>
          <th>dst_bytes</th>
          <th>land</th>
          <th>wrong_fragment</th>
          <th>urgent</th>
          <th>hot</th>
          <th>...</th>
          <th>dst_host_same_srv_rate</th>
          <th>dst_host_diff_srv_rate</th>
          <th>dst_host_same_src_port_rate</th>
          <th>dst_host_srv_diff_host_rate</th>
          <th>dst_host_serror_rate</th>
          <th>dst_host_srv_serror_rate</th>
          <th>dst_host_rerror_rate</th>
          <th>dst_host_srv_rerror_rate</th>
          <th>label</th>
          <th>difficulty level</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0</td>
          <td>tcp</td>
          <td>ftp_data</td>
          <td>SF</td>
          <td>491</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>0.17</td>
          <td>0.03</td>
          <td>0.17</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.05</td>
          <td>0.00</td>
          <td>normal</td>
          <td>20</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0</td>
          <td>udp</td>
          <td>other</td>
          <td>SF</td>
          <td>146</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>0.00</td>
          <td>0.60</td>
          <td>0.88</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>normal</td>
          <td>15</td>
        </tr>
        <tr>
          <th>2</th>
          <td>0</td>
          <td>tcp</td>
          <td>private</td>
          <td>S0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>0.10</td>
          <td>0.05</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>neptune</td>
          <td>19</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0</td>
          <td>tcp</td>
          <td>http</td>
          <td>SF</td>
          <td>232</td>
          <td>8153</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>1.00</td>
          <td>0.00</td>
          <td>0.03</td>
          <td>0.04</td>
          <td>0.03</td>
          <td>0.01</td>
          <td>0.00</td>
          <td>0.01</td>
          <td>normal</td>
          <td>21</td>
        </tr>
        <tr>
          <th>4</th>
          <td>0</td>
          <td>tcp</td>
          <td>http</td>
          <td>SF</td>
          <td>199</td>
          <td>420</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>1.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>normal</td>
          <td>21</td>
        </tr>
      </tbody>
    </table>
    <p>5 rows × 43 columns</p>
    </div>



.. code:: ipython3

    # Printing the first few lines of Testing data
    testingData.head()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>duration</th>
          <th>protocol_type</th>
          <th>service</th>
          <th>flag</th>
          <th>src_bytes</th>
          <th>dst_bytes</th>
          <th>land</th>
          <th>wrong_fragment</th>
          <th>urgent</th>
          <th>hot</th>
          <th>...</th>
          <th>dst_host_same_srv_rate</th>
          <th>dst_host_diff_srv_rate</th>
          <th>dst_host_same_src_port_rate</th>
          <th>dst_host_srv_diff_host_rate</th>
          <th>dst_host_serror_rate</th>
          <th>dst_host_srv_serror_rate</th>
          <th>dst_host_rerror_rate</th>
          <th>dst_host_srv_rerror_rate</th>
          <th>label</th>
          <th>difficulty level</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0</td>
          <td>tcp</td>
          <td>private</td>
          <td>REJ</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>0.04</td>
          <td>0.06</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>neptune</td>
          <td>21</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0</td>
          <td>tcp</td>
          <td>private</td>
          <td>REJ</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>0.00</td>
          <td>0.06</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.00</td>
          <td>1.00</td>
          <td>neptune</td>
          <td>21</td>
        </tr>
        <tr>
          <th>2</th>
          <td>2</td>
          <td>tcp</td>
          <td>ftp_data</td>
          <td>SF</td>
          <td>12983</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>0.61</td>
          <td>0.04</td>
          <td>0.61</td>
          <td>0.02</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>normal</td>
          <td>21</td>
        </tr>
        <tr>
          <th>3</th>
          <td>0</td>
          <td>icmp</td>
          <td>eco_i</td>
          <td>SF</td>
          <td>20</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>1.00</td>
          <td>0.00</td>
          <td>1.00</td>
          <td>0.28</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.00</td>
          <td>0.00</td>
          <td>saint</td>
          <td>15</td>
        </tr>
        <tr>
          <th>4</th>
          <td>1</td>
          <td>tcp</td>
          <td>telnet</td>
          <td>RSTO</td>
          <td>0</td>
          <td>15</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>...</td>
          <td>0.31</td>
          <td>0.17</td>
          <td>0.03</td>
          <td>0.02</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.83</td>
          <td>0.71</td>
          <td>mscan</td>
          <td>11</td>
        </tr>
      </tbody>
    </table>
    <p>5 rows × 43 columns</p>
    </div>



.. code:: ipython3

    # Printing the information for the Training data
    trainingData.info()


.. parsed-literal::

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 125973 entries, 0 to 125972
    Data columns (total 43 columns):
     #   Column                       Non-Null Count   Dtype  
    ---  ------                       --------------   -----  
     0   duration                     125973 non-null  int64  
     1   protocol_type                125973 non-null  object 
     2   service                      125973 non-null  object 
     3   flag                         125973 non-null  object 
     4   src_bytes                    125973 non-null  int64  
     5   dst_bytes                    125973 non-null  int64  
     6   land                         125973 non-null  int64  
     7   wrong_fragment               125973 non-null  int64  
     8   urgent                       125973 non-null  int64  
     9   hot                          125973 non-null  int64  
     10  num_failed_logins            125973 non-null  int64  
     11  logged_in                    125973 non-null  int64  
     12  num_compromised              125973 non-null  int64  
     13  root_shell                   125973 non-null  int64  
     14  su_attempted                 125973 non-null  int64  
     15  num_root                     125973 non-null  int64  
     16  num_file_creations           125973 non-null  int64  
     17  num_shells                   125973 non-null  int64  
     18  num_access_files             125973 non-null  int64  
     19  num_outbound_cmds            125973 non-null  int64  
     20  is_host_login                125973 non-null  int64  
     21  is_guest_login               125973 non-null  int64  
     22  count                        125973 non-null  int64  
     23  srv_count                    125973 non-null  int64  
     24  serror_rate                  125973 non-null  float64
     25  srv_serror_rate              125973 non-null  float64
     26  rerror_rate                  125973 non-null  float64
     27  srv_rerror_rate              125973 non-null  float64
     28  same_srv_rate                125973 non-null  float64
     29  diff_srv_rate                125973 non-null  float64
     30  srv_diff_host_rate           125973 non-null  float64
     31  dst_host_count               125973 non-null  int64  
     32  dst_host_srv_count           125973 non-null  int64  
     33  dst_host_same_srv_rate       125973 non-null  float64
     34  dst_host_diff_srv_rate       125973 non-null  float64
     35  dst_host_same_src_port_rate  125973 non-null  float64
     36  dst_host_srv_diff_host_rate  125973 non-null  float64
     37  dst_host_serror_rate         125973 non-null  float64
     38  dst_host_srv_serror_rate     125973 non-null  float64
     39  dst_host_rerror_rate         125973 non-null  float64
     40  dst_host_srv_rerror_rate     125973 non-null  float64
     41  label                        125973 non-null  object 
     42  difficulty level             125973 non-null  int64  
    dtypes: float64(15), int64(24), object(4)
    memory usage: 41.3+ MB


.. code:: ipython3

    # Printing the information for the Testing data
    testingData.info()


.. parsed-literal::

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 22544 entries, 0 to 22543
    Data columns (total 43 columns):
     #   Column                       Non-Null Count  Dtype  
    ---  ------                       --------------  -----  
     0   duration                     22544 non-null  int64  
     1   protocol_type                22544 non-null  object 
     2   service                      22544 non-null  object 
     3   flag                         22544 non-null  object 
     4   src_bytes                    22544 non-null  int64  
     5   dst_bytes                    22544 non-null  int64  
     6   land                         22544 non-null  int64  
     7   wrong_fragment               22544 non-null  int64  
     8   urgent                       22544 non-null  int64  
     9   hot                          22544 non-null  int64  
     10  num_failed_logins            22544 non-null  int64  
     11  logged_in                    22544 non-null  int64  
     12  num_compromised              22544 non-null  int64  
     13  root_shell                   22544 non-null  int64  
     14  su_attempted                 22544 non-null  int64  
     15  num_root                     22544 non-null  int64  
     16  num_file_creations           22544 non-null  int64  
     17  num_shells                   22544 non-null  int64  
     18  num_access_files             22544 non-null  int64  
     19  num_outbound_cmds            22544 non-null  int64  
     20  is_host_login                22544 non-null  int64  
     21  is_guest_login               22544 non-null  int64  
     22  count                        22544 non-null  int64  
     23  srv_count                    22544 non-null  int64  
     24  serror_rate                  22544 non-null  float64
     25  srv_serror_rate              22544 non-null  float64
     26  rerror_rate                  22544 non-null  float64
     27  srv_rerror_rate              22544 non-null  float64
     28  same_srv_rate                22544 non-null  float64
     29  diff_srv_rate                22544 non-null  float64
     30  srv_diff_host_rate           22544 non-null  float64
     31  dst_host_count               22544 non-null  int64  
     32  dst_host_srv_count           22544 non-null  int64  
     33  dst_host_same_srv_rate       22544 non-null  float64
     34  dst_host_diff_srv_rate       22544 non-null  float64
     35  dst_host_same_src_port_rate  22544 non-null  float64
     36  dst_host_srv_diff_host_rate  22544 non-null  float64
     37  dst_host_serror_rate         22544 non-null  float64
     38  dst_host_srv_serror_rate     22544 non-null  float64
     39  dst_host_rerror_rate         22544 non-null  float64
     40  dst_host_srv_rerror_rate     22544 non-null  float64
     41  label                        22544 non-null  object 
     42  difficulty level             22544 non-null  int64  
    dtypes: float64(15), int64(24), object(4)
    memory usage: 7.4+ MB


.. code:: ipython3

    # Descriptive statistics on Training Data
    trainingData.describe()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>duration</th>
          <th>src_bytes</th>
          <th>dst_bytes</th>
          <th>land</th>
          <th>wrong_fragment</th>
          <th>urgent</th>
          <th>hot</th>
          <th>num_failed_logins</th>
          <th>logged_in</th>
          <th>num_compromised</th>
          <th>...</th>
          <th>dst_host_srv_count</th>
          <th>dst_host_same_srv_rate</th>
          <th>dst_host_diff_srv_rate</th>
          <th>dst_host_same_src_port_rate</th>
          <th>dst_host_srv_diff_host_rate</th>
          <th>dst_host_serror_rate</th>
          <th>dst_host_srv_serror_rate</th>
          <th>dst_host_rerror_rate</th>
          <th>dst_host_srv_rerror_rate</th>
          <th>difficulty level</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>count</th>
          <td>125973.00000</td>
          <td>1.259730e+05</td>
          <td>1.259730e+05</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>...</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
          <td>125973.000000</td>
        </tr>
        <tr>
          <th>mean</th>
          <td>287.14465</td>
          <td>4.556674e+04</td>
          <td>1.977911e+04</td>
          <td>0.000198</td>
          <td>0.022687</td>
          <td>0.000111</td>
          <td>0.204409</td>
          <td>0.001222</td>
          <td>0.395736</td>
          <td>0.279250</td>
          <td>...</td>
          <td>115.653005</td>
          <td>0.521242</td>
          <td>0.082951</td>
          <td>0.148379</td>
          <td>0.032542</td>
          <td>0.284452</td>
          <td>0.278485</td>
          <td>0.118832</td>
          <td>0.120240</td>
          <td>19.504060</td>
        </tr>
        <tr>
          <th>std</th>
          <td>2604.51531</td>
          <td>5.870331e+06</td>
          <td>4.021269e+06</td>
          <td>0.014086</td>
          <td>0.253530</td>
          <td>0.014366</td>
          <td>2.149968</td>
          <td>0.045239</td>
          <td>0.489010</td>
          <td>23.942042</td>
          <td>...</td>
          <td>110.702741</td>
          <td>0.448949</td>
          <td>0.188922</td>
          <td>0.308997</td>
          <td>0.112564</td>
          <td>0.444784</td>
          <td>0.445669</td>
          <td>0.306557</td>
          <td>0.319459</td>
          <td>2.291503</td>
        </tr>
        <tr>
          <th>min</th>
          <td>0.00000</td>
          <td>0.000000e+00</td>
          <td>0.000000e+00</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>...</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
        </tr>
        <tr>
          <th>25%</th>
          <td>0.00000</td>
          <td>0.000000e+00</td>
          <td>0.000000e+00</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>...</td>
          <td>10.000000</td>
          <td>0.050000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>18.000000</td>
        </tr>
        <tr>
          <th>50%</th>
          <td>0.00000</td>
          <td>4.400000e+01</td>
          <td>0.000000e+00</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>...</td>
          <td>63.000000</td>
          <td>0.510000</td>
          <td>0.020000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>20.000000</td>
        </tr>
        <tr>
          <th>75%</th>
          <td>0.00000</td>
          <td>2.760000e+02</td>
          <td>5.160000e+02</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>0.000000</td>
          <td>...</td>
          <td>255.000000</td>
          <td>1.000000</td>
          <td>0.070000</td>
          <td>0.060000</td>
          <td>0.020000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>21.000000</td>
        </tr>
        <tr>
          <th>max</th>
          <td>42908.00000</td>
          <td>1.379964e+09</td>
          <td>1.309937e+09</td>
          <td>1.000000</td>
          <td>3.000000</td>
          <td>3.000000</td>
          <td>77.000000</td>
          <td>5.000000</td>
          <td>1.000000</td>
          <td>7479.000000</td>
          <td>...</td>
          <td>255.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>21.000000</td>
        </tr>
      </tbody>
    </table>
    <p>8 rows × 39 columns</p>
    </div>



.. code:: ipython3

    # Descriptive statistics on Testing Data
    testingData.describe()




.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>duration</th>
          <th>src_bytes</th>
          <th>dst_bytes</th>
          <th>land</th>
          <th>wrong_fragment</th>
          <th>urgent</th>
          <th>hot</th>
          <th>num_failed_logins</th>
          <th>logged_in</th>
          <th>num_compromised</th>
          <th>...</th>
          <th>dst_host_srv_count</th>
          <th>dst_host_same_srv_rate</th>
          <th>dst_host_diff_srv_rate</th>
          <th>dst_host_same_src_port_rate</th>
          <th>dst_host_srv_diff_host_rate</th>
          <th>dst_host_serror_rate</th>
          <th>dst_host_srv_serror_rate</th>
          <th>dst_host_rerror_rate</th>
          <th>dst_host_srv_rerror_rate</th>
          <th>difficulty level</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>count</th>
          <td>22544.000000</td>
          <td>2.254400e+04</td>
          <td>2.254400e+04</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>...</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
          <td>22544.000000</td>
        </tr>
        <tr>
          <th>mean</th>
          <td>218.859076</td>
          <td>1.039545e+04</td>
          <td>2.056019e+03</td>
          <td>0.000311</td>
          <td>0.008428</td>
          <td>0.000710</td>
          <td>0.105394</td>
          <td>0.021647</td>
          <td>0.442202</td>
          <td>0.119899</td>
          <td>...</td>
          <td>140.750532</td>
          <td>0.608722</td>
          <td>0.090540</td>
          <td>0.132261</td>
          <td>0.019638</td>
          <td>0.097814</td>
          <td>0.099426</td>
          <td>0.233385</td>
          <td>0.226683</td>
          <td>18.017965</td>
        </tr>
        <tr>
          <th>std</th>
          <td>1407.176612</td>
          <td>4.727864e+05</td>
          <td>2.121930e+04</td>
          <td>0.017619</td>
          <td>0.142599</td>
          <td>0.036473</td>
          <td>0.928428</td>
          <td>0.150328</td>
          <td>0.496659</td>
          <td>7.269597</td>
          <td>...</td>
          <td>111.783972</td>
          <td>0.435688</td>
          <td>0.220717</td>
          <td>0.306268</td>
          <td>0.085394</td>
          <td>0.273139</td>
          <td>0.281866</td>
          <td>0.387229</td>
          <td>0.400875</td>
          <td>4.270361</td>
        </tr>
        <tr>
          <th>min</th>
          <td>0.000000</td>
          <td>0.000000e+00</td>
          <td>0.000000e+00</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>...</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
        </tr>
        <tr>
          <th>25%</th>
          <td>0.000000</td>
          <td>0.000000e+00</td>
          <td>0.000000e+00</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>...</td>
          <td>15.000000</td>
          <td>0.070000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>17.000000</td>
        </tr>
        <tr>
          <th>50%</th>
          <td>0.000000</td>
          <td>5.400000e+01</td>
          <td>4.600000e+01</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>...</td>
          <td>168.000000</td>
          <td>0.920000</td>
          <td>0.010000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>20.000000</td>
        </tr>
        <tr>
          <th>75%</th>
          <td>0.000000</td>
          <td>2.870000e+02</td>
          <td>6.010000e+02</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>1.000000</td>
          <td>0.000000</td>
          <td>...</td>
          <td>255.000000</td>
          <td>1.000000</td>
          <td>0.060000</td>
          <td>0.030000</td>
          <td>0.010000</td>
          <td>0.000000</td>
          <td>0.000000</td>
          <td>0.360000</td>
          <td>0.170000</td>
          <td>21.000000</td>
        </tr>
        <tr>
          <th>max</th>
          <td>57715.000000</td>
          <td>6.282565e+07</td>
          <td>1.345927e+06</td>
          <td>1.000000</td>
          <td>3.000000</td>
          <td>3.000000</td>
          <td>101.000000</td>
          <td>4.000000</td>
          <td>1.000000</td>
          <td>796.000000</td>
          <td>...</td>
          <td>255.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>1.000000</td>
          <td>21.000000</td>
        </tr>
      </tbody>
    </table>
    <p>8 rows × 39 columns</p>
    </div>



.. code:: ipython3

    # Checking for missing values in Training data
    trainingData.isnull().sum()




.. parsed-literal::

    duration                       0
    protocol_type                  0
    service                        0
    flag                           0
    src_bytes                      0
    dst_bytes                      0
    land                           0
    wrong_fragment                 0
    urgent                         0
    hot                            0
    num_failed_logins              0
    logged_in                      0
    num_compromised                0
    root_shell                     0
    su_attempted                   0
    num_root                       0
    num_file_creations             0
    num_shells                     0
    num_access_files               0
    num_outbound_cmds              0
    is_host_login                  0
    is_guest_login                 0
    count                          0
    srv_count                      0
    serror_rate                    0
    srv_serror_rate                0
    rerror_rate                    0
    srv_rerror_rate                0
    same_srv_rate                  0
    diff_srv_rate                  0
    srv_diff_host_rate             0
    dst_host_count                 0
    dst_host_srv_count             0
    dst_host_same_srv_rate         0
    dst_host_diff_srv_rate         0
    dst_host_same_src_port_rate    0
    dst_host_srv_diff_host_rate    0
    dst_host_serror_rate           0
    dst_host_srv_serror_rate       0
    dst_host_rerror_rate           0
    dst_host_srv_rerror_rate       0
    label                          0
    difficulty level               0
    dtype: int64



.. code:: ipython3

    # Checking for missing values in Testing data
    testingData.isnull().sum()




.. parsed-literal::

    duration                       0
    protocol_type                  0
    service                        0
    flag                           0
    src_bytes                      0
    dst_bytes                      0
    land                           0
    wrong_fragment                 0
    urgent                         0
    hot                            0
    num_failed_logins              0
    logged_in                      0
    num_compromised                0
    root_shell                     0
    su_attempted                   0
    num_root                       0
    num_file_creations             0
    num_shells                     0
    num_access_files               0
    num_outbound_cmds              0
    is_host_login                  0
    is_guest_login                 0
    count                          0
    srv_count                      0
    serror_rate                    0
    srv_serror_rate                0
    rerror_rate                    0
    srv_rerror_rate                0
    same_srv_rate                  0
    diff_srv_rate                  0
    srv_diff_host_rate             0
    dst_host_count                 0
    dst_host_srv_count             0
    dst_host_same_srv_rate         0
    dst_host_diff_srv_rate         0
    dst_host_same_src_port_rate    0
    dst_host_srv_diff_host_rate    0
    dst_host_serror_rate           0
    dst_host_srv_serror_rate       0
    dst_host_rerror_rate           0
    dst_host_srv_rerror_rate       0
    label                          0
    difficulty level               0
    dtype: int64



.. code:: ipython3

    # Verifying duplicates in the Training data
    trainingData.duplicated().sum()




.. parsed-literal::

    np.int64(0)



.. code:: ipython3

    # Verifying duplicates in the Testing data
    testingData.duplicated().sum()




.. parsed-literal::

    np.int64(0)



.. code:: ipython3

    # Removing duplicates from Training data and testing data
    trainingData.drop_duplicates(inplace=True)
    testingData.drop_duplicates(inplace=True)

.. code:: ipython3

    # Value counts for the 'label' column in training data
    trainingData['label'].value_counts()




.. parsed-literal::

    label
    normal             67343
    neptune            41214
    satan               3633
    ipsweep             3599
    portsweep           2931
    smurf               2646
    nmap                1493
    back                 956
    teardrop             892
    warezclient          890
    pod                  201
    guess_passwd          53
    buffer_overflow       30
    warezmaster           20
    land                  18
    imap                  11
    rootkit               10
    loadmodule             9
    ftp_write              8
    multihop               7
    phf                    4
    perl                   3
    spy                    2
    Name: count, dtype: int64



.. code:: ipython3

    # Value counts for the 'label' column in testing data
    testingData['label'].value_counts()




.. parsed-literal::

    label
    normal             9711
    neptune            4657
    guess_passwd       1231
    mscan               996
    warezmaster         944
    apache2             737
    satan               735
    processtable        685
    smurf               665
    back                359
    snmpguess           331
    saint               319
    mailbomb            293
    snmpgetattack       178
    portsweep           157
    ipsweep             141
    httptunnel          133
    nmap                 73
    pod                  41
    buffer_overflow      20
    multihop             18
    named                17
    ps                   15
    sendmail             14
    xterm                13
    rootkit              13
    teardrop             12
    xlock                 9
    land                  7
    xsnoop                4
    ftp_write             3
    loadmodule            2
    worm                  2
    perl                  2
    sqlattack             2
    udpstorm              2
    phf                   2
    imap                  1
    Name: count, dtype: int64



.. code:: ipython3

    # Visualizing the distribution of attack types in training data
    plt.figure(figsize=(15, 7))
    sns.countplot(data=trainingData, y='label')
    plt.title('Distribution of Attack Types in Training Data')
    plt.show()



.. image:: output_18_0.png


.. code:: ipython3

    # Exploring the top 20 distribution of 'protocol_type' in testing data
    TopProtocolTypeinTestingData = testingData['protocol_type'].value_counts().head(20)
    plt.figure(figsize=(10, 5))
    sns.countplot(data=testingData.reset_index(), y='protocol_type', order=TopProtocolTypeinTestingData.index)
    plt.title('Top 20 Protocol Types in Testing Data')
    plt.show()



.. image:: output_19_0.png


.. code:: ipython3

    # Exploring the distribution of 'logged_in' in testing data
    plt.figure(figsize=(10, 4))
    sns.countplot(data=testingData, x='logged_in')
    plt.title('Distribution of Logged In Status in Testing Data')
    plt.show()



.. image:: output_20_0.png


.. code:: ipython3

    # Exploring the distribution of 'service' in training data by top 5 services
    plt.figure(figsize=(12, 8))
    sns.countplot(data=trainingData.reset_index(), y='service', order=trainingData['service'].value_counts().iloc[:10].index)
    plt.title('Top 10 Service Distributions in Training Data')
    plt.show()



.. image:: output_21_0.png


.. code:: ipython3

    # Plotting Box Plot of Duration by Attack Type
    plt.figure(figsize=(12, 6))
    sns.boxplot(x='label', y='duration', data=trainingData)
    plt.xticks(rotation=90)
    plt.title('Box Plot of Duration by Attack Type')
    plt.show()



.. image:: output_22_0.png


.. code:: ipython3

    # Plotting a Pair plot of a subset of numerical features
    PairPlotFeatures = ['duration', 'src_bytes', 'dst_bytes', 'count', 'label']
    sns.pairplot(trainingData[PairPlotFeatures].sample(frac=0.005), hue='label') # Using small sample
    plt.suptitle('Pair Plot of Subset Features by Attack Type', y=1.02)
    plt.show()



.. image:: output_23_0.png


.. code:: ipython3

    # Plotting Stacked bar plot for protocol_type and label
    TotalProtocols = trainingData.groupby(['protocol_type', 'label']).size().unstack(fill_value=0)
    TotalProtocols.plot(kind='bar', stacked=True, figsize=(15, 8))
    plt.title('Attack Types by Protocol Type')
    plt.xlabel('Protocol Type')
    plt.ylabel('Count')
    plt.xticks(rotation=90)
    plt.legend(title='Attack Type')
    plt.show()



.. image:: output_24_0.png


.. code:: ipython3

    # Plotting a histogram with KDE for Distribution of Duration
    plt.figure(figsize=(10, 6))
    sns.histplot(data=trainingData, x='duration', kde=True, bins=50)
    plt.title('Distribution of Duration')
    plt.show()



.. image:: output_25_0.png


.. code:: ipython3

    # Plotting the Correlation matrix of numerical features in training data
    plt.figure(figsize=(15, 12))
    sns.heatmap(trainingData.select_dtypes(include=np.number).corr(), annot=False, cmap='coolwarm')
    plt.title('Correlation Matrix of Numerical Features in Training Data')
    plt.show()



.. image:: output_26_0.png


.. code:: ipython3

    # Identify object columns excluding the 'label' column
    obj_cols = trainingData.drop('label', axis=1).select_dtypes(include='object').columns

.. code:: ipython3

    attack_mapping = {
        # Normal
        'normal': 'normal',
    
        # DoS: Denial of service attack
        'back': 'dos', 'land': 'dos', 'neptune': 'dos', 'pod': 'dos', 'smurf': 'dos',
        'teardrop': 'dos', 'apache2': 'dos', 'udpstorm': 'dos', 'processtable': 'dos', 'worm': 'dos', 'mailbomb': 'dos',
    
        # Probe: Probing attacks 
        'satan': 'probe', 'ipsweep': 'probe', 'nmap': 'probe', 'portsweep': 'probe',
        'mscan': 'probe', 'saint': 'probe',
    
        # R2L: Root to local attacks
        'guess_passwd': 'r2l', 'ftp_write': 'r2l', 'imap': 'r2l', 'phf': 'r2l',
        'multihop': 'r2l', 'warezmaster': 'r2l', 'warezclient': 'r2l', 'spy': 'r2l',
        'xlock': 'r2l', 'xsnoop': 'r2l', 'snmpguess': 'r2l', 'snmpgetattack': 'r2l',
        'httptunnel': 'r2l', 'sendmail': 'r2l', 'named': 'r2l',
    
        # U2R: User to Root attack
        'buffer_overflow': 'u2r', 'loadmodule': 'u2r', 'rootkit': 'u2r',
        'perl': 'u2r', 'sqlattack': 'u2r', 'xterm': 'u2r', 'ps': 'u2r'
    }


.. code:: ipython3

    # Apply LabelEncoder on both train and test
    for col in obj_cols:
        le = LabelEncoder()
        combined = pd.concat([trainingData[col], testingData[col]], axis=0)
        le.fit(combined)
        trainingData[col] = le.transform(trainingData[col])
        testingData[col] = le.transform(testingData[col])

.. code:: ipython3

    # Scaling the data
    scaler = StandardScaler()
    
    X_train = trainingData.drop('label', axis=1)
    y_train = trainingData['label']
    X_test = testingData.drop('label', axis=1)
    y_test = testingData['label']
    
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

.. code:: ipython3

    # Reshape for 1D convolutional layers
    X_train_seq = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
    X_test_seq  = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

.. code:: ipython3

    # Map original labels to 5-class labels
    y_train = y_train.map(attack_mapping)
    y_test = y_test.map(attack_mapping)
    
    # After mapping
    unmapped_train = y_train[~y_train.isin(attack_mapping.keys())].unique()
    unmapped_test = y_test[~y_test.isin(attack_mapping.keys())].unique()
    
    print("Unmapped in train:", unmapped_train)
    print("Unmapped in test:", unmapped_test)
    



.. parsed-literal::

    Unmapped in train: ['dos' 'r2l' 'probe' 'u2r']
    Unmapped in test: ['dos' 'probe' 'r2l' 'u2r']


.. code:: ipython3

    # Encoding the data 
    encoder = LabelEncoder()
    y_train_enc = encoder.fit_transform(y_train)
    y_test_enc = encoder.transform(y_test)
    
    y_train_cat = to_categorical(y_train_enc)
    y_test_cat = to_categorical(y_test_enc)

.. code:: ipython3

    # Building the CNN + Bi-LSTM model
    model = Sequential()
    model.add(Conv1D(filters=96, kernel_size=5, activation='relu', input_shape=(X_train_seq.shape[1], 1)))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Bidirectional(LSTM(48, return_sequences=False, dropout=0.5, recurrent_dropout=0.2)))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(y_train_cat.shape[1], activation='softmax'))


.. parsed-literal::

    C:\Users\hitar\Music\Vandit Project\ids\Lib\site-packages\keras\src\layers\convolutional\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
      super().__init__(activity_regularizer=activity_regularizer, **kwargs)


.. code:: ipython3

    # Compiling the Model
    
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    
    history = model.fit(X_train_seq, y_train_cat,
                        epochs=50,
                        batch_size=64,
                        validation_data=(X_test_seq, y_test_cat)) 
                       


.. parsed-literal::

    Epoch 1/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m29s[0m 12ms/step - accuracy: 0.9560 - loss: 0.1315 - val_accuracy: 0.8284 - val_loss: 0.9036
    Epoch 2/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m24s[0m 12ms/step - accuracy: 0.9882 - loss: 0.0366 - val_accuracy: 0.7897 - val_loss: 1.2998
    Epoch 3/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9916 - loss: 0.0250 - val_accuracy: 0.7484 - val_loss: 1.5467
    Epoch 4/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9933 - loss: 0.0205 - val_accuracy: 0.8018 - val_loss: 1.4380
    Epoch 5/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9941 - loss: 0.0180 - val_accuracy: 0.7854 - val_loss: 1.6844
    Epoch 6/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9948 - loss: 0.0164 - val_accuracy: 0.7800 - val_loss: 1.5955
    Epoch 7/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9952 - loss: 0.0146 - val_accuracy: 0.7473 - val_loss: 1.9366
    Epoch 8/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m28s[0m 14ms/step - accuracy: 0.9955 - loss: 0.0134 - val_accuracy: 0.7655 - val_loss: 1.9547
    Epoch 9/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m28s[0m 14ms/step - accuracy: 0.9956 - loss: 0.0128 - val_accuracy: 0.7580 - val_loss: 1.9428
    Epoch 10/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m28s[0m 14ms/step - accuracy: 0.9958 - loss: 0.0118 - val_accuracy: 0.7636 - val_loss: 2.0365
    Epoch 11/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m28s[0m 14ms/step - accuracy: 0.9960 - loss: 0.0117 - val_accuracy: 0.7674 - val_loss: 2.1399
    Epoch 12/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9963 - loss: 0.0109 - val_accuracy: 0.7579 - val_loss: 2.3113
    Epoch 13/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9961 - loss: 0.0111 - val_accuracy: 0.7744 - val_loss: 2.1652
    Epoch 14/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9964 - loss: 0.0095 - val_accuracy: 0.7599 - val_loss: 2.6041
    Epoch 15/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9967 - loss: 0.0093 - val_accuracy: 0.7748 - val_loss: 2.2302
    Epoch 16/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9970 - loss: 0.0093 - val_accuracy: 0.7807 - val_loss: 2.4163
    Epoch 17/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9970 - loss: 0.0089 - val_accuracy: 0.7722 - val_loss: 2.7222
    Epoch 18/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9972 - loss: 0.0082 - val_accuracy: 0.7742 - val_loss: 2.9540
    Epoch 19/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9970 - loss: 0.0092 - val_accuracy: 0.7565 - val_loss: 2.8341
    Epoch 20/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9972 - loss: 0.0083 - val_accuracy: 0.7528 - val_loss: 2.8105
    Epoch 21/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9976 - loss: 0.0075 - val_accuracy: 0.7548 - val_loss: 3.5792
    Epoch 22/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9974 - loss: 0.0076 - val_accuracy: 0.7646 - val_loss: 3.1974
    Epoch 23/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9976 - loss: 0.0076 - val_accuracy: 0.7642 - val_loss: 2.8528
    Epoch 24/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9976 - loss: 0.0073 - val_accuracy: 0.7586 - val_loss: 3.4620
    Epoch 25/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9975 - loss: 0.0075 - val_accuracy: 0.7690 - val_loss: 3.5140
    Epoch 26/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9975 - loss: 0.0075 - val_accuracy: 0.7569 - val_loss: 3.4463
    Epoch 27/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9976 - loss: 0.0069 - val_accuracy: 0.7636 - val_loss: 3.2301
    Epoch 28/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9979 - loss: 0.0064 - val_accuracy: 0.7685 - val_loss: 3.5051
    Epoch 29/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9978 - loss: 0.0076 - val_accuracy: 0.7702 - val_loss: 3.1928
    Epoch 30/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9978 - loss: 0.0065 - val_accuracy: 0.7579 - val_loss: 3.6562
    Epoch 31/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9977 - loss: 0.0068 - val_accuracy: 0.7698 - val_loss: 3.4973
    Epoch 32/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9977 - loss: 0.0069 - val_accuracy: 0.7554 - val_loss: 3.6909
    Epoch 33/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m25s[0m 13ms/step - accuracy: 0.9977 - loss: 0.0069 - val_accuracy: 0.7696 - val_loss: 3.4093
    Epoch 34/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9980 - loss: 0.0064 - val_accuracy: 0.7673 - val_loss: 3.3724
    Epoch 35/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9979 - loss: 0.0065 - val_accuracy: 0.7540 - val_loss: 3.6962
    Epoch 36/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9979 - loss: 0.0056 - val_accuracy: 0.7620 - val_loss: 3.9301
    Epoch 37/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9977 - loss: 0.0067 - val_accuracy: 0.7712 - val_loss: 3.6707
    Epoch 38/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9980 - loss: 0.0061 - val_accuracy: 0.7615 - val_loss: 3.9000
    Epoch 39/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9979 - loss: 0.0062 - val_accuracy: 0.7698 - val_loss: 3.6669
    Epoch 40/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9978 - loss: 0.0064 - val_accuracy: 0.7612 - val_loss: 3.9405
    Epoch 41/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9979 - loss: 0.0067 - val_accuracy: 0.7675 - val_loss: 3.4814
    Epoch 42/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9979 - loss: 0.0061 - val_accuracy: 0.7463 - val_loss: 4.1528
    Epoch 43/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9980 - loss: 0.0059 - val_accuracy: 0.7649 - val_loss: 3.9242
    Epoch 44/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 13ms/step - accuracy: 0.9980 - loss: 0.0057 - val_accuracy: 0.7647 - val_loss: 3.8914
    Epoch 45/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9978 - loss: 0.0062 - val_accuracy: 0.7740 - val_loss: 3.8834
    Epoch 46/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m26s[0m 13ms/step - accuracy: 0.9980 - loss: 0.0055 - val_accuracy: 0.7714 - val_loss: 3.9996
    Epoch 47/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m27s[0m 14ms/step - accuracy: 0.9980 - loss: 0.0058 - val_accuracy: 0.7762 - val_loss: 3.9880
    Epoch 48/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m28s[0m 14ms/step - accuracy: 0.9980 - loss: 0.0055 - val_accuracy: 0.7689 - val_loss: 4.3135
    Epoch 49/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m28s[0m 14ms/step - accuracy: 0.9981 - loss: 0.0056 - val_accuracy: 0.7704 - val_loss: 3.9224
    Epoch 50/50
    [1m1969/1969[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m28s[0m 14ms/step - accuracy: 0.9981 - loss: 0.0062 - val_accuracy: 0.7862 - val_loss: 3.4263


.. code:: ipython3

    # Calculate precision/recall/F1 for CNN-BiLSTM
    y_pred_cnn = np.argmax(model.predict(X_test_seq, batch_size=64), axis=-1)
    y_test_cnn = np.argmax(y_test_cat, axis=-1)
    accuracy_cnn = accuracy_score(y_test_cnn, y_pred_cnn)
    precision_cnn = precision_score(y_test_cnn, y_pred_cnn, average='weighted' )
    recall_cnn = recall_score(y_test_cnn, y_pred_cnn, average='weighted')
    f1_cnn = f1_score(y_test_cnn, y_pred_cnn, average='weighted')
    print(f"CNN-BiLSTM Model Evaluation:")
    print(f"Accuracy: {accuracy_cnn:.4f}")
    print(f"Precision: {precision_cnn:.4f}")
    print(f"Recall: {recall_cnn:.4f}")
    print(f"F1-score: {f1_cnn:.4f}")



.. parsed-literal::

    [1m353/353[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 3ms/step
    CNN-BiLSTM Model Evaluation:
    Accuracy: 0.7862
    Precision: 0.8151
    Recall: 0.7862
    F1-score: 0.7680


.. code:: ipython3

    # Evaluating the performance ofthe model
    
    # Accuracy and loss plots
    plt.plot(history.history['accuracy'], label='Train Acc')
    plt.plot(history.history['val_accuracy'], label='Val Acc')
    plt.title("Model Accuracy")
    plt.legend()
    plt.show()
    
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title("Model Loss")
    plt.legend()
    plt.show()



.. image:: output_37_0.png



.. image:: output_37_1.png


Model training Random Forest model.
-----------------------------------

.. code:: ipython3

    # Instantiate and train the Random Forest model
    rf_model = RandomForestClassifier(n_estimators=50, max_depth=1, random_state=32)
    rf_model.fit(X_train_scaled, y_train_enc)
    
    # Make predictions on the test data
    y_pred_rf = rf_model.predict(X_test_scaled)

Evaluate random forest model
----------------------------

.. code:: ipython3

    accuracy_rf = accuracy_score(y_test_enc, y_pred_rf)
    precision_rf = precision_score(y_test_enc, y_pred_rf, average='weighted')
    recall_rf = recall_score(y_test_enc, y_pred_rf, average='weighted')
    f1_rf = f1_score(y_test_enc, y_pred_rf, average='weighted')
    
    print(f"Random Forest Model Evaluation:")
    print(f"Accuracy: {accuracy_rf:.4f}")
    print(f"Precision: {precision_rf:.4f}")
    print(f"Recall: {recall_rf:.4f}")
    print(f"F1-score: {f1_rf:.4f}")


.. parsed-literal::

    Random Forest Model Evaluation:
    Accuracy: 0.6374
    Precision: 0.4994
    Recall: 0.6374
    F1-score: 0.5474


.. parsed-literal::

    C:\Users\hitar\Music\Vandit Project\ids\Lib\site-packages\sklearn\metrics\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
      _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])


Define and train xgboost model
------------------------------

.. code:: ipython3

    # Create an XGBClassifier instance
    xgb_model = XGBClassifier(use_label_encoder=False, max_depth=1, n_estimators=50, eval_metric='mlogloss', random_state=32)
    
    # Fit the xgb_model to the training data
    xgb_model.fit(X_train_scaled, y_train_enc)
    
    # Make predictions on the testing data
    y_pred_xgb = xgb_model.predict(X_test_scaled)


.. parsed-literal::

    C:\Users\hitar\Music\Vandit Project\ids\Lib\site-packages\xgboost\training.py:183: UserWarning: [13:58:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738: 
    Parameters: { "use_label_encoder" } are not used.
    
      bst.update(dtrain, iteration=i, fobj=obj)


Evaluate xgboost model
----------------------

.. code:: ipython3

    accuracy_xgb = accuracy_score(y_test_enc, y_pred_xgb)
    precision_xgb = precision_score(y_test_enc, y_pred_xgb, average='weighted')
    recall_xgb = recall_score(y_test_enc, y_pred_xgb, average='weighted')
    f1_xgb = f1_score(y_test_enc, y_pred_xgb, average='weighted')
    
    print(f"XGBoost Model Evaluation:")
    print(f"Accuracy: {accuracy_xgb:.4f}")
    print(f"Precision: {precision_xgb:.4f}")
    print(f"Recall: {recall_xgb:.4f}")
    print(f"F1-score: {f1_xgb:.4f}")


.. parsed-literal::

    XGBoost Model Evaluation:
    Accuracy: 0.7769
    Precision: 0.7919
    Recall: 0.7769
    F1-score: 0.7287


.. code:: ipython3

    # Function to measure latency per sample
    def measure_latency_per_sample(model, X_test):
        start_time = time.time()
        model.predict(X_test)
        end_time = time.time()
        total_time = end_time - start_time
        latency_per_sample = total_time / X_test.shape[0]
        return latency_per_sample
    
    # Measure latency per prediction for each model
    latency_cnn = measure_latency_per_sample(model, X_test_seq)*1000
    latency_rf = measure_latency_per_sample(rf_model, X_test_scaled)*1000
    latency_xgb = measure_latency_per_sample(xgb_model, X_test_scaled)*1000
    
    print(f"CNN-BiLSTM Latency per prediction: {latency_cnn:.4f} ms")
    print(f"Random Forest Latency per prediction: {latency_rf:.4f} ms")
    print(f"XGBoost Latency per prediction: {latency_xgb:.4f} ms")


.. parsed-literal::

    [1m705/705[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 2ms/step
    CNN-BiLSTM Latency per prediction: 0.0668 ms
    Random Forest Latency per prediction: 0.0008 ms
    XGBoost Latency per prediction: 0.0002 ms


.. code:: ipython3

    # Comparing models performance including latency
    models = ['CNN-BiLSTM', 'Random Forest', 'XGBoost']
    precisions = [precision_cnn, precision_rf, precision_xgb]
    recalls = [recall_cnn, recall_rf, recall_xgb]
    f1_scores = [f1_cnn, f1_rf, f1_xgb]
    latencies = [latency_cnn, latency_rf, latency_xgb]
    accuracy = [accuracy_cnn, accuracy_rf, accuracy_xgb]
    
    x = np.arange(len(models))
    width = 0.175
    
    fig, ax = plt.subplots(figsize=(14, 10))
    rects1 = ax.bar(x - width, precisions, width, label='Precision')
    rects2 = ax.bar(x, recalls, width, label='Recall')
    rects3 = ax.bar(x + width, f1_scores, width, label='F1-score')
    rects4 = ax.bar(x + 3 * width, latencies, width, label='Latency (seconds)')
    rects5 = ax.bar(x + 2 * width, accuracy, width, label='Accuracy')
    
    ax.set_ylabel('Score/Time')
    ax.set_title('Model Performance Comparison including Latency')
    ax.set_xticks(x + width / 2)
    ax.set_xticklabels(models)
    ax.legend()
    
    plt.show()



.. image:: output_47_0.png


.. code:: ipython3

    # Performing predictions using all models as real vs prediction
    predictions = {
        'CNN-BiLSTM': y_pred_cnn,
        'Random Forest': y_pred_rf,
        'XGBoost': y_pred_xgb
    }
    real_repeated = np.tile(y_test_enc, len(predictions))
    predicted = []
    model_names_repeated = []
    for model_name, model_predictions in predictions.items():
        predicted.extend(model_predictions)
        model_names_repeated.extend([model_name] * len(model_predictions))
    df = pd.DataFrame({
        'Real': real_repeated,
        'Predicted': predicted,
        'Model': model_names_repeated
    })
    # Add a column to mark mispredictions
    df['Correct'] = df['Real'] == df['Predicted']
    
    # Show only misclassified rows
    misclassified_df = df[df['Correct'] == False]
    
    # Display top 10 misclassifications
    
    display(df.head(10))



.. raw:: html

    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Real</th>
          <th>Predicted</th>
          <th>Model</th>
          <th>Correct</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>0</td>
          <td>0</td>
          <td>CNN-BiLSTM</td>
          <td>True</td>
        </tr>
        <tr>
          <th>1</th>
          <td>0</td>
          <td>0</td>
          <td>CNN-BiLSTM</td>
          <td>True</td>
        </tr>
        <tr>
          <th>2</th>
          <td>1</td>
          <td>1</td>
          <td>CNN-BiLSTM</td>
          <td>True</td>
        </tr>
        <tr>
          <th>3</th>
          <td>2</td>
          <td>2</td>
          <td>CNN-BiLSTM</td>
          <td>True</td>
        </tr>
        <tr>
          <th>4</th>
          <td>2</td>
          <td>1</td>
          <td>CNN-BiLSTM</td>
          <td>False</td>
        </tr>
        <tr>
          <th>5</th>
          <td>1</td>
          <td>1</td>
          <td>CNN-BiLSTM</td>
          <td>True</td>
        </tr>
        <tr>
          <th>6</th>
          <td>1</td>
          <td>1</td>
          <td>CNN-BiLSTM</td>
          <td>True</td>
        </tr>
        <tr>
          <th>7</th>
          <td>3</td>
          <td>1</td>
          <td>CNN-BiLSTM</td>
          <td>False</td>
        </tr>
        <tr>
          <th>8</th>
          <td>1</td>
          <td>1</td>
          <td>CNN-BiLSTM</td>
          <td>True</td>
        </tr>
        <tr>
          <th>9</th>
          <td>3</td>
          <td>2</td>
          <td>CNN-BiLSTM</td>
          <td>False</td>
        </tr>
      </tbody>
    </table>
    </div>


.. code:: ipython3

    models_predictions = {
        'CNN-BiLSTM': y_pred_cnn,
        'Random Forest': y_pred_rf,
        'XGBoost': y_pred_xgb
    }
    
    for model_name, y_pred in models_predictions.items():
        cm = confusion_matrix(y_test_enc, y_pred)
        print(f"Confusion Matrix for {model_name}:")
        
        # Optional: Heatmap visualization
        plt.figure(figsize=(6,5))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'{model_name} Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.show()


.. parsed-literal::

    Confusion Matrix for CNN-BiLSTM:



.. image:: output_49_1.png


.. parsed-literal::

    Confusion Matrix for Random Forest:



.. image:: output_49_3.png


.. parsed-literal::

    Confusion Matrix for XGBoost:



.. image:: output_49_5.png


